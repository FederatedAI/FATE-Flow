dag:
  conf:
    cores: 8
  parties:
  - party_id: ['9999']
    role: guest
  - party_id: ['10000']
    role: host
  - party_id: ['10000']
    role: arbiter
  party_tasks:
    guest_9999:
      parties:
      - party_id: ['9999']
        role: guest
      tasks:
        nn_0:
          inputs:
            data:
              train_data:
                data_warehouse:
                  name: breast_homo_guest
                  namespace: experiment
                  roles: [guest]
          parameters:
            runner_conf:
              algo: fedavg
              data_collator_conf: null
              dataset_conf: null
              fed_args_conf: {aggregate_freq: 1, aggregate_strategy: epochs, aggregator: secure_aggregate}
              loss_conf:
                item_name: BCELoss
                kwargs: {reduce: null, reduction: mean, size_average: null, weight: null}
                module_name: torch.nn
              model_conf:
                item_name: load_seq
                kwargs:
                  seq_conf:
                    0:
                      item_name: Linear
                      kwargs: {bias: true, device: null, dtype: null, in_features: 30,
                        out_features: 16}
                      module_name: torch.nn
                    1:
                      item_name: ReLU
                      kwargs: {inplace: false}
                      module_name: torch.nn
                    2:
                      item_name: Linear
                      kwargs: {bias: true, device: null, dtype: null, in_features: 16,
                        out_features: 1}
                      module_name: torch.nn
                    3:
                      item_name: Sigmoid
                      kwargs: {}
                      module_name: torch.nn
                module_name: fate.components.components.nn.torch.base
              optimizer_conf:
                item_name: Adam
                kwargs:
                  amsgrad: false
                  betas: [0.9, 0.999]
                  eps: 1e-08
                  lr: 0.01
                  weight_decay: 0
                module_name: torch.optim
              task_type: binary
              tokenizer_conf: null
              training_args_conf:
                deepspeed:
                  fp16: {enabled: false}
                  optimizer:
                    params: {lr: 0.0005}
                    type: Adam
                  scheduler:
                    params: {warmup_min_lr: 0}
                    type: WarmupLR
                  train_batch_size: auto
                  train_micro_batch_size_per_gpu: 16
                  zero_optimization: {allgather_bucket_size: 500000000.0, allgather_partitions: true,
                    contiguous_gradients: true, overlap_comm: false, reduce_bucket_size: 500000000.0,
                    reduce_scatter: true, stage: 3, stage3_gather_16bit_weights_on_model_save: true}
                disable_tqdm: true
                logging_strategy: epoch
                lr_scheduler_type: constant
                num_train_epochs: 10
                per_device_train_batch_size: 64
                save_strategy: no
                seed: 114514
          conf:
            launcher_name: deepspeed
            engine_run:
              cores: 4
    host_10000:
      parties:
      - party_id: ['10000']
        role: host
      tasks:
        nn_0:
          inputs:
            data:
              train_data:
                data_warehouse:
                  name: breast_homo_host
                  namespace: experiment
                  roles: [host]
          parameters:
            runner_conf:
              algo: fedavg
              data_collator_conf: null
              dataset_conf: null
              fed_args_conf: {aggregate_freq: 1, aggregate_strategy: epochs, aggregator: secure_aggregate}
              loss_conf:
                item_name: BCELoss
                kwargs: {reduce: null, reduction: mean, size_average: null, weight: null}
                module_name: torch.nn
              model_conf:
                item_name: load_seq
                kwargs:
                  seq_conf:
                    0:
                      item_name: Linear
                      kwargs: {bias: true, device: null, dtype: null, in_features: 30,
                        out_features: 16}
                      module_name: torch.nn
                    1:
                      item_name: ReLU
                      kwargs: {inplace: false}
                      module_name: torch.nn
                    2:
                      item_name: Linear
                      kwargs: {bias: true, device: null, dtype: null, in_features: 16,
                        out_features: 1}
                      module_name: torch.nn
                    3:
                      item_name: Sigmoid
                      kwargs: {}
                      module_name: torch.nn
                module_name: fate.components.components.nn.torch.base
              optimizer_conf:
                item_name: Adam
                kwargs:
                  amsgrad: false
                  betas: [0.9, 0.999]
                  eps: 1e-08
                  lr: 0.01
                  weight_decay: 0
                module_name: torch.optim
              task_type: binary
              tokenizer_conf: null
              training_args_conf:
                deepspeed:
                  fp16: {enabled: false}
                  optimizer:
                    params: {lr: 0.0005}
                    type: Adam
                  scheduler:
                    params: {warmup_min_lr: 0}
                    type: WarmupLR
                  train_batch_size: auto
                  train_micro_batch_size_per_gpu: 16
                  zero_optimization: {allgather_bucket_size: 500000000.0, allgather_partitions: true,
                    contiguous_gradients: true, overlap_comm: false, reduce_bucket_size: 500000000.0,
                    reduce_scatter: true, stage: 3, stage3_gather_16bit_weights_on_model_save: true}
                disable_tqdm: true
                logging_strategy: epoch
                lr_scheduler_type: constant
                num_train_epochs: 10
                per_device_train_batch_size: 64
                save_strategy: no
                seed: 114514
          conf:
            launcher_name: deepspeed
            engine_run:
              cores: 4
  stage: train
  tasks:
    eval_0:
      component_ref: evaluation
      dependent_tasks: [nn_0]
      inputs:
        data:
          input_data:
            task_output_artifact:
            - output_artifact_key: train_data_output
              producer_task: nn_0
              roles: [guest]
      parameters:
        label_column_name: null
        metrics: [auc]
        predict_column_name: null
      parties:
      - party_id: ['9999']
        role: guest
      stage: default
    nn_0:
      component_ref: homo_nn
      inputs:
        data: {}
        model: {}
      parameters:
        runner_conf:
          algo: fedavg
          data_collator_conf: null
          dataset_conf: null
          fed_args_conf: {aggregate_freq: 1, aggregate_strategy: epochs, aggregator: secure_aggregate}
          loss_conf:
            item_name: BCELoss
            kwargs: {reduce: null, reduction: mean, size_average: null, weight: null}
            module_name: torch.nn
          model_conf:
            item_name: load_seq
            kwargs:
              seq_conf:
                0:
                  item_name: Linear
                  kwargs: {bias: true, device: null, dtype: null, in_features: 30,
                    out_features: 16}
                  module_name: torch.nn
                1:
                  item_name: ReLU
                  kwargs: {inplace: false}
                  module_name: torch.nn
                2:
                  item_name: Linear
                  kwargs: {bias: true, device: null, dtype: null, in_features: 16,
                    out_features: 1}
                  module_name: torch.nn
                3:
                  item_name: Sigmoid
                  kwargs: {}
                  module_name: torch.nn
            module_name: fate.components.components.nn.torch.base
          optimizer_conf:
            item_name: Adam
            kwargs:
              amsgrad: false
              betas: [0.9, 0.999]
              eps: 1e-08
              lr: 0.01
              weight_decay: 0
            module_name: torch.optim
          task_type: binary
          tokenizer_conf: null
          training_args_conf:
            deepspeed:
              fp16: {enabled: false}
              optimizer:
                params: {lr: 0.0005}
                type: Adam
              scheduler:
                params: {warmup_min_lr: 0}
                type: WarmupLR
              train_batch_size: auto
              train_micro_batch_size_per_gpu: 16
              zero_optimization: {allgather_bucket_size: 500000000.0, allgather_partitions: true,
                contiguous_gradients: true, overlap_comm: false, reduce_bucket_size: 500000000.0,
                reduce_scatter: true, stage: 3, stage3_gather_16bit_weights_on_model_save: true}
            disable_tqdm: true
            logging_strategy: epoch
            lr_scheduler_type: constant
            num_train_epochs: 10
            per_device_train_batch_size: 64
            save_strategy: no
            seed: 114514
schema_version: 2.0.0.beta


